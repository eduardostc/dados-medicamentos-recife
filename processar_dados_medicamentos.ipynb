{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unificação e Limpeza de Dados da API do Recife\n",
    "\n",
    "Este notebook realiza a extração, unificação e limpeza de dados de múltiplas APIs disponibilizadas pelo portal de dados da cidade do Recife.\n",
    "\n",
    "## Estrutura do Código\n",
    "\n",
    "1. **Importação de Bibliotecas**:\n",
    "   - O código utiliza as bibliotecas `requests`, `pandas`, `urllib`, `re` e `ftfy`.\n",
    "   - O pacote `requests` é usado para fazer requisições HTTP à API.\n",
    "   - `pandas` é utilizado para manipulação de dados em formato de DataFrame.\n",
    "   - `urllib` e `re` ajudam na limpeza e manipulação de strings.\n",
    "   - `ftfy` é usado para corrigir problemas de codificação em strings.\n",
    "\n",
    "2. **Definição de Resource IDs**:\n",
    "   - Uma lista de `resource_ids` é criada para acessar diferentes conjuntos de dados na API.\n",
    "\n",
    "3. **Função `fetch_data`**:\n",
    "   - Esta função é responsável por buscar dados de cada `resource_id` utilizando paginação.\n",
    "   - Os dados são armazenados em uma lista e convertidos em um DataFrame.\n",
    "\n",
    "4. **Unificação de DataFrames**:\n",
    "   - Os DataFrames de cada `resource_id` são concatenados em um único DataFrame.\n",
    "\n",
    "5. **Limpeza de Dados**:\n",
    "   - A função `clean_text` é utilizada para corrigir e limpar texto, removendo caracteres indesejados e corrigindo problemas de codificação.\n",
    "   - Um dicionário de `correcoes` é criado para substituir palavras específicas que estão com erros.\n",
    "\n",
    "6. **Salvamento do DataFrame**:\n",
    "   - O DataFrame unificado e limpo é salvo como um arquivo Excel (`.xlsx`) na pasta padrão do Google Colab.\n",
    "\n",
    "## Execução\n",
    "- Ao executar este código, você obterá um arquivo Excel contendo os dados unificados e corrigidos, que pode ser baixado diretamente do Colab.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in c:\\programdata\\anaconda3\\lib\\site-packages (2.0.3)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (2.31.0)\n",
      "Requirement already satisfied: ftfy in c:\\users\\carlos.e.santos\\appdata\\roaming\\python\\python311\\site-packages (6.2.3)\n",
      "Requirement already satisfied: openpyxl in c:\\programdata\\anaconda3\\lib\\site-packages (3.0.10)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (1.24.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (2023.7.22)\n",
      "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in c:\\users\\carlos.e.santos\\appdata\\roaming\\python\\python311\\site-packages (from ftfy) (0.2.13)\n",
      "Requirement already satisfied: et_xmlfile in c:\\programdata\\anaconda3\\lib\\site-packages (from openpyxl) (1.1.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Erro ao carregar dados do resource_id 6e98bbef-0eae-4729-a822-d9089598c04c: 404 Client Error: Not Found for url: http://dados.recife.pe.gov.br/api/3/action/datastore_search?resource_id=6e98bbef-0eae-4729-a822-d9089598c04c&limit=1000&offset=0\n",
      "Erro ao carregar dados do resource_id 97803604-a662-4d6f-b579-144ac8a3daab: 404 Client Error: Not Found for url: http://dados.recife.pe.gov.br/api/3/action/datastore_search?resource_id=97803604-a662-4d6f-b579-144ac8a3daab&limit=1000&offset=0\n",
      "Erro ao carregar dados do resource_id 2ca525fa-4290-4654-9551-caedf31a512c: 404 Client Error: Not Found for url: http://dados.recife.pe.gov.br/api/3/action/datastore_search?resource_id=2ca525fa-4290-4654-9551-caedf31a512c&limit=1000&offset=0\n",
      "Erro ao carregar dados do resource_id 74347e0f-cc13-4319-93ee-7eb1fcf621d0: 404 Client Error: Not Found for url: http://dados.recife.pe.gov.br/api/3/action/datastore_search?resource_id=74347e0f-cc13-4319-93ee-7eb1fcf621d0&limit=1000&offset=0\n",
      "Erro ao carregar dados do resource_id 82e3b1cf-44bb-477e-a447-b32c6be5694d: 404 Client Error: Not Found for url: http://dados.recife.pe.gov.br/api/3/action/datastore_search?resource_id=82e3b1cf-44bb-477e-a447-b32c6be5694d&limit=1000&offset=0\n",
      "Erro ao carregar dados do resource_id d1c31571-0e7f-4c10-9bdb-a9e10f9e56b0: 404 Client Error: Not Found for url: http://dados.recife.pe.gov.br/api/3/action/datastore_search?resource_id=d1c31571-0e7f-4c10-9bdb-a9e10f9e56b0&limit=1000&offset=0\n",
      "Erro ao carregar dados do resource_id ef6bbbdf-708d-40f4-a0eb-03a793d293ab: 404 Client Error: Not Found for url: http://dados.recife.pe.gov.br/api/3/action/datastore_search?resource_id=ef6bbbdf-708d-40f4-a0eb-03a793d293ab&limit=1000&offset=0\n",
      "Dados unificados com sucesso!\n",
      "   _id  distrito                                            unidade  \\\n",
      "0    1         4  Us 106 CS Prof Joaquim Cavalcante             ...   \n",
      "1    2         4  Us 106 CS Prof Joaquim Cavalcante             ...   \n",
      "2    3         4  Us 106 CS Prof Joaquim Cavalcante             ...   \n",
      "3    4         4  Us 106 CS Prof Joaquim Cavalcante             ...   \n",
      "4    5         4  Us 106 CS Prof Joaquim Cavalcante             ...   \n",
      "\n",
      "                                              classe  \\\n",
      "0  AGENTE ANTIRESSORTIVO                         ...   \n",
      "1  AGENTE URICOSÃšRICO                           ...   \n",
      "2  AINE                                          ...   \n",
      "3  AINE/ ANTICOAGULANTE                          ...   \n",
      "4  ANALGÃ‰SICO/ ANTIPIRÃ‰TICO                    ...   \n",
      "\n",
      "                                        apresentacao  \\\n",
      "0  COMPR                                         ...   \n",
      "1  COMPR                                         ...   \n",
      "2  AMP                                           ...   \n",
      "3  COMPR                                         ...   \n",
      "4  AMP                                           ...   \n",
      "\n",
      "                                        tipo_produto  codigo_produto  \\\n",
      "0  MEDICAMENTOS                                  ...           10651   \n",
      "1  MEDICAMENTOS                                  ...           10669   \n",
      "2  MEDICAMENTOS                                  ...           10057   \n",
      "3  MEDICAMENTOS                                  ...           10003   \n",
      "4  MEDICAMENTOS                                  ...           10111   \n",
      "\n",
      "                                             produto    cadum  quantidade  \n",
      "0  ALENDRONATO DE SODIO 70MG                     ...  30068.0         268  \n",
      "1  ALOPURINOL 300MG                              ...  30069.0         520  \n",
      "2  CETOPROFENO 50MG/ML SOLUCAO INJETAVEL - 2ML   ...  30048.0           3  \n",
      "3  ACIDO ACETILSALICILICO 100MG                  ...  30064.0        7914  \n",
      "4  DIPIRONA 500MG/ML SOLUCAO INJETAVEL - 2ML     ...  30099.0           0  \n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Cannot save file into a non-existent directory: '\\content\\drive\\MyDrive\\Colab Notebooks\\Medicamentos'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 103\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;66;03m# Salvar o DataFrame unificado como arquivo Excel ('.xlsx')\u001b[39;00m\n\u001b[0;32m    102\u001b[0m output_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive/MyDrive/Colab Notebooks/Medicamentos/dados_unificados.xlsx\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 103\u001b[0m \u001b[43munified_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_excel\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDados salvos com sucesso no arquivo \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\carlos.e.santos\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\generic.py:2345\u001b[0m, in \u001b[0;36mNDFrame.to_excel\u001b[1;34m(self, excel_writer, sheet_name, na_rep, float_format, columns, header, index, index_label, startrow, startcol, engine, merge_cells, inf_rep, freeze_panes, storage_options, engine_kwargs)\u001b[0m\n\u001b[0;32m   2332\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mformats\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexcel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ExcelFormatter\n\u001b[0;32m   2334\u001b[0m formatter \u001b[38;5;241m=\u001b[39m ExcelFormatter(\n\u001b[0;32m   2335\u001b[0m     df,\n\u001b[0;32m   2336\u001b[0m     na_rep\u001b[38;5;241m=\u001b[39mna_rep,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2343\u001b[0m     inf_rep\u001b[38;5;241m=\u001b[39minf_rep,\n\u001b[0;32m   2344\u001b[0m )\n\u001b[1;32m-> 2345\u001b[0m \u001b[43mformatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2346\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexcel_writer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2347\u001b[0m \u001b[43m    \u001b[49m\u001b[43msheet_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msheet_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstartrow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstartrow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstartcol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstartcol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfreeze_panes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfreeze_panes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2352\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2354\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\carlos.e.santos\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\formats\\excel.py:946\u001b[0m, in \u001b[0;36mExcelFormatter.write\u001b[1;34m(self, writer, sheet_name, startrow, startcol, freeze_panes, engine, storage_options, engine_kwargs)\u001b[0m\n\u001b[0;32m    942\u001b[0m     need_save \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    943\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    944\u001b[0m     \u001b[38;5;66;03m# error: Cannot instantiate abstract class 'ExcelWriter' with abstract\u001b[39;00m\n\u001b[0;32m    945\u001b[0m     \u001b[38;5;66;03m# attributes 'engine', 'save', 'supported_extensions' and 'write_cells'\u001b[39;00m\n\u001b[1;32m--> 946\u001b[0m     writer \u001b[38;5;241m=\u001b[39m \u001b[43mExcelWriter\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[abstract]\u001b[39;49;00m\n\u001b[0;32m    947\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    948\u001b[0m \u001b[43m        \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    949\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    950\u001b[0m \u001b[43m        \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    951\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    952\u001b[0m     need_save \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    954\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\carlos.e.santos\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\excel\\_xlsxwriter.py:205\u001b[0m, in \u001b[0;36mXlsxWriter.__init__\u001b[1;34m(self, path, engine, date_format, datetime_format, mode, storage_options, if_sheet_exists, engine_kwargs, **kwargs)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAppend mode is not supported with xlsxwriter!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 205\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    207\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    208\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    209\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdatetime_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdatetime_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    210\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    211\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    212\u001b[0m \u001b[43m    \u001b[49m\u001b[43mif_sheet_exists\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mif_sheet_exists\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    213\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_book \u001b[38;5;241m=\u001b[39m Workbook(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handles\u001b[38;5;241m.\u001b[39mhandle, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mengine_kwargs)\n",
      "File \u001b[1;32mc:\\Users\\carlos.e.santos\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:1263\u001b[0m, in \u001b[0;36mExcelWriter.__init__\u001b[1;34m(self, path, engine, date_format, datetime_format, mode, storage_options, if_sheet_exists, engine_kwargs)\u001b[0m\n\u001b[0;32m   1259\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handles \u001b[38;5;241m=\u001b[39m IOHandles(\n\u001b[0;32m   1260\u001b[0m     cast(IO[\u001b[38;5;28mbytes\u001b[39m], path), compression\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m}\n\u001b[0;32m   1261\u001b[0m )\n\u001b[0;32m   1262\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, ExcelWriter):\n\u001b[1;32m-> 1263\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1264\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m   1265\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1266\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cur_sheet \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1268\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m date_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\carlos.e.santos\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\common.py:739\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    737\u001b[0m \u001b[38;5;66;03m# Only for write methods\u001b[39;00m\n\u001b[0;32m    738\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m is_path:\n\u001b[1;32m--> 739\u001b[0m     \u001b[43mcheck_parent_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    741\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compression:\n\u001b[0;32m    742\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m compression \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzstd\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    743\u001b[0m         \u001b[38;5;66;03m# compression libraries do not like an explicit text-mode\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\carlos.e.santos\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\common.py:604\u001b[0m, in \u001b[0;36mcheck_parent_directory\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    602\u001b[0m parent \u001b[38;5;241m=\u001b[39m Path(path)\u001b[38;5;241m.\u001b[39mparent\n\u001b[0;32m    603\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parent\u001b[38;5;241m.\u001b[39mis_dir():\n\u001b[1;32m--> 604\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124mrf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot save file into a non-existent directory: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparent\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mOSError\u001b[0m: Cannot save file into a non-existent directory: '\\content\\drive\\MyDrive\\Colab Notebooks\\Medicamentos'"
     ]
    }
   ],
   "source": [
    "# Instalar bibliotecas necessárias (caso não estejam instaladas)\n",
    "!pip install pandas requests ftfy openpyxl\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import urllib.parse\n",
    "import re\n",
    "import ftfy  # Importando o pacote ftfy para corrigir a codificação das strings\n",
    "\n",
    "# Lista dos 8 resource_ids (links da API)\n",
    "resource_ids = [\n",
    "    '6e98bbef-0eae-4729-a822-d9089598c04c',\n",
    "    '97803604-a662-4d6f-b579-144ac8a3daab',\n",
    "    '2ca525fa-4290-4654-9551-caedf31a512c',\n",
    "    '1c41117a-679b-417d-9649-68813efd9b6b',\n",
    "    '74347e0f-cc13-4319-93ee-7eb1fcf621d0',\n",
    "    '82e3b1cf-44bb-477e-a447-b32c6be5694d',\n",
    "    'd1c31571-0e7f-4c10-9bdb-a9e10f9e56b0',\n",
    "    'ef6bbbdf-708d-40f4-a0eb-03a793d293ab'\n",
    "]\n",
    "\n",
    "# Função para carregar todos os dados de um resource_id com PAGINAÇÃO\n",
    "def fetch_data(resource_id):\n",
    "    base_url = f'http://dados.recife.pe.gov.br/api/3/action/datastore_search?resource_id={resource_id}'\n",
    "    all_records = []\n",
    "    offset = 0\n",
    "    limit = 1000  # Limite de registros por página\n",
    "    while True:\n",
    "        url = f'{base_url}&limit={limit}&offset={offset}'\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            # Decodificar a resposta JSON corretamente\n",
    "            data = response.json()\n",
    "\n",
    "            records = data['result']['records']\n",
    "\n",
    "            if not records:\n",
    "                break  # Se não houver mais registros, interrompe a iteração\n",
    "\n",
    "            all_records.extend(records)  # Adiciona os registros à lista total\n",
    "            offset += limit  # Incrementa o offset para a próxima página\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao carregar dados do resource_id {resource_id}: {e}\")\n",
    "            return None\n",
    "\n",
    "    df = pd.DataFrame(all_records)\n",
    "    return df\n",
    "\n",
    "# Lista para armazenar os DataFrames de cada link\n",
    "dataframes = []\n",
    "\n",
    "# Iterar sobre os resource_ids e buscar os dados\n",
    "for resource_id in resource_ids:\n",
    "    df = fetch_data(resource_id)\n",
    "    if df is not None:\n",
    "        dataframes.append(df)\n",
    "\n",
    "# Concatenar todos os DataFrames em um único\n",
    "if dataframes:\n",
    "    unified_df = pd.concat(dataframes, ignore_index=True)\n",
    "    print(\"Dados unificados com sucesso!\")\n",
    "else:\n",
    "    print(\"Nenhum dado foi carregado.\")\n",
    "\n",
    "# Exibir as primeiras linhas dos dados unificados\n",
    "if 'unified_df' in locals():\n",
    "    print(unified_df.head())\n",
    "\n",
    "# Função para limpar e corrigir caracteres mal codificados\n",
    "def clean_text(text):\n",
    "    if isinstance(text, str):\n",
    "        text = urllib.parse.unquote(text)  # Decodificar texto que possa estar com encoding de URL\n",
    "        text = ftfy.fix_text(text)  # Corrigir caracteres mal codificados\n",
    "        text = re.sub(r'[^\\w\\sÁÉÍÓÚáéíóúâêîôûãõç]', '', text)  # Remover caracteres não alfanuméricos, exceto acentos e cedilha\n",
    "        text = re.sub(r'Ã', 'A', text)  # Substituir 'Ã' por 'A'\n",
    "        text = re.sub(r'¢', 'ç', text)  # Substituir '¢' por 'ç'\n",
    "        text = re.sub(r'Í', 'I', text)  # Substituir 'Í' por 'I'\n",
    "        text = re.sub(r'Ç', 'C', text)  # Substituir 'Ç' por 'C'\n",
    "        return text.strip()  # Remover espaços em branco extras\n",
    "    return text\n",
    "\n",
    "# Aplicar a função de limpeza na coluna 'classe'\n",
    "if 'classe' in unified_df.columns:\n",
    "    unified_df['classe'] = unified_df['classe'].apply(clean_text)\n",
    "\n",
    "# Dicionário de substituições para corrigir palavras específicas\n",
    "correcoes = {\n",
    "    'ANSIOLATICO BENZODIAZEPANICO': 'ANSIOLÍTICO BENZODIAZEPÍNICO',\n",
    "    'ANTIALÉRGICO ANTIHISTAMANICO': 'ANTIALÉRGICO ANTI-HISTAMÍNICO',\n",
    "    'ANTIARRATMICO': 'ANTIARRÍTMICO',\n",
    "    # Adicione mais correções conforme necessário\n",
    "}\n",
    "\n",
    "# Aplicar as substituições no DataFrame\n",
    "if 'classe' in unified_df.columns:\n",
    "    unified_df['classe'] = unified_df['classe'].replace(correcoes)\n",
    "\n",
    "# Salvar o DataFrame unificado como arquivo Excel ('.xlsx')\n",
    "output_file = '/content/drive/MyDrive/Colab Notebooks/Medicamentos/dados_unificados.xlsx'\n",
    "unified_df.to_excel(output_file, index=False)\n",
    "\n",
    "print(f\"Dados salvos com sucesso no arquivo '{output_file}'!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#1- Etapa de \"Extração\":Coleta de dados de várias fontes e Carregamento e Exibição Inicial dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Etapa 1: Carregamento e Exibição Inicial dos Dados\n",
    "import pandas as pd\n",
    "\n",
    "# Definir o caminho do arquivo no Google Drive\n",
    "file_path = '/content/drive/MyDrive/Colab Notebooks/Medicamentos/dados_unificados.xlsx'\n",
    "\n",
    "# Carregar o arquivo CSV unificado\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Exibir as primeiras 5 linhas do DataFrame\n",
    "print(df.head(10))\n",
    "\n",
    "# Exibir as colunas disponíveis no DataFrame\n",
    "# print(\"\\nColunas disponíveis:\")\n",
    "print(df.columns)\n",
    "\n",
    "# Exibir informações sobre o DataFrame (tipos de dados e valores nulos)\n",
    "# print(\"\\nInformações gerais do DataFrame:\")\n",
    "print(df.info())\n",
    "\n",
    "# Exibir a quantidade de valores ausentes por coluna\n",
    "# print(\"\\nValores ausentes por coluna:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Exibir estatísticas descritivas para colunas numéricas\n",
    "# print(\"\\nEstatísticas descritivas para colunas numéricas:\")\n",
    "print(df.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#2- Etapa de Transformação: Limpeza de Dados Ausentes\n",
    "Nessa etapa, lidamos com valores ausentes na coluna 'cadum', removendo linhas onde este valor está ausente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Etapa 2: Limpeza de Dados Ausentes\n",
    "print(\"\\nValores ausentes por coluna antes da limpeza:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Remover linhas com valores ausentes na coluna 'cadum'\n",
    "df = df.dropna(subset=['cadum'])\n",
    "\n",
    "print(\"\\nValores ausentes por coluna após o tratamento:\")\n",
    "print(df.isnull().sum())\n",
    "print(\"\\nEstatísticas descritivas da coluna 'cadum' após o tratamento:\")\n",
    "print(df['cadum'].describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#3-Etapa de Transformação dos dados: Conversão de Tipos de Dados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Certifique-se de que o DataFrame 'df' está definido e contém os dados necessários\n",
    "\n",
    "# Verificar o tipo de dados atual da coluna 'cadum'\n",
    "print(\"\\nTipos de dados atuais:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# Converter 'cadum' para inteiro, lidando com valores ausentes (se necessário)\n",
    "# Aqui, usamos `fillna()` para substituir NaNs por um valor antes da conversão\n",
    "df['cadum'] = df['cadum'].fillna(0).astype(int)\n",
    "\n",
    "# Converter colunas do tipo 'object' para 'string'\n",
    "df['unidade'] = df['unidade'].astype('string')\n",
    "df['classe'] = df['classe'].astype('string')\n",
    "df['apresentacao'] = df['apresentacao'].astype('string')\n",
    "df['tipo_produto'] = df['tipo_produto'].astype('string')\n",
    "df['produto'] = df['produto'].astype('string')\n",
    "\n",
    "# Verificar se a conversão foi realizada corretamente\n",
    "print(\"\\nInformações gerais após conversão:\")\n",
    "print(df.info())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Etapa 4: Tratamento de Dados Duplicados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Etapa 4: Tratamento de Dados Duplicados\n",
    "\n",
    "# Exibir o número de linhas antes de remover duplicatas\n",
    "print(f\"\\nNúmero de linhas antes de remover duplicatas: {df.shape[0]}\")\n",
    "\n",
    "# Verificar se há registros duplicados\n",
    "duplicated_rows = df[df.duplicated()]\n",
    "print(f\"\\nNúmero de linhas duplicadas: {len(duplicated_rows)}\")\n",
    "\n",
    "# Remover duplicatas\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# Verificar se as duplicatas foram removidas\n",
    "print(f\"\\nNúmero de linhas após remover duplicatas: {df.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Etapa 5: Tratamento de Valores Anômalos (nº negativos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Etapa 5: Tratamento de Valores Anômalos\n",
    "\n",
    "# Exibir estatísticas descritivas para a coluna 'quantidade'\n",
    "print(\"\\nEstatísticas descritivas da coluna 'quantidade':\")\n",
    "print(df['quantidade'].describe())\n",
    "\n",
    "# Corrigir valores negativos na coluna 'quantidade'\n",
    "# Substituir valores negativos por 0 (presumindo que valores negativos são erros)\n",
    "df['quantidade'] = df['quantidade'].apply(lambda x: 0 if x < 0 else x)\n",
    "\n",
    "# Exibir estatísticas descritivas da coluna 'quantidade' após correção\n",
    "print(\"\\nEstatísticas descritivas da coluna 'quantidade' após correção:\")\n",
    "print(df['quantidade'].describe())\n",
    "\n",
    "# Identificar potenciais outliers\n",
    "# Exemplo: definir um limite para identificar outlier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6- Etapa de Identificação de Outliers\n",
    "```\n",
    "**OPCIONAL** ◀\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Etapa 6: Identificação de Outliers\n",
    "\n",
    "# Calcular o IQR para a coluna 'quantidade'\n",
    "Q1 = df['quantidade'].quantile(0.25)\n",
    "Q3 = df['quantidade'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Definir os limites inferior e superior para identificar outliers\n",
    "limite_inferior = Q1 - 1.5 * IQR\n",
    "limite_superior = Q3 + 1.5 * IQR\n",
    "\n",
    "# Identificar outliers na coluna 'quantidade'\n",
    "df['outlier_quantidade'] = df['quantidade'].apply(lambda x: 'Outlier' if x < limite_inferior or x > limite_superior else 'Normal')\n",
    "\n",
    "# Exibir estatísticas descritivas da coluna 'quantidade' para verificação\n",
    "print(\"\\nEstatísticas descritivas da coluna 'quantidade':\")\n",
    "print(df['quantidade'].describe())\n",
    "\n",
    "# Exibir a quantidade de outliers identificados\n",
    "num_outliers = df['outlier_quantidade'].value_counts().get('Outlier', 0)\n",
    "print(f\"\\nNúmero de outliers identificados: {num_outliers}\")\n",
    "\n",
    "# Exibir uma amostra dos dados com a nova coluna de identificação de outliers\n",
    "print(\"\\nAmostra dos dados com identificação de outliers:\")\n",
    "print(df[['quantidade', 'outlier_quantidade']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**#(Não precisa executar caso faça no BI)Etapa 7 :Análise de Estoque por Unidade, Classe e Tipo de Produto (Insight)**\n",
    " -Os insights como estoque_por_unidade, qtd_total_produtosPorclasse, estoque_por_tipo_produto, estoque_por_distrito e unidades_baixo_estoque são cálculos e análises feitos com os dados, mas eles não são adicionados ao DataFrame original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Etapa 9: Análise de Estoque por Unidade, Classe e Tipo de Produto\n",
    "\n",
    "# Estoque total por unidade\n",
    "estoque_por_unidade = df.groupby('unidade')['quantidade'].sum().reset_index() #Primeira linha: Retorna um DataFrame onde 'unidade' é uma coluna.\n",
    "#estoque_por_unidade = df[[\"unidade\",\"quantidade\"]].groupby(\"unidade\").sum() #Segunda linha: Retorna um DataFrame onde 'unidade' é o índice.\n",
    "print(\"\\nEstoque total por unidade:\")\n",
    "print(estoque_por_unidade)\n",
    "\n",
    "# Quantidade total de produtos por classe\n",
    "qtd_total_produtosPorclasse = df.groupby('classe')['quantidade'].sum().reset_index()\n",
    "print(\"\\nEstoque total por classe:\")\n",
    "print(qtd_total_produtosPorclasse)\n",
    "\n",
    "# Quantidade total por tipo de produto\n",
    "estoque_por_tipo_produto = df.groupby('tipo_produto')['quantidade'].sum().reset_index()\n",
    "print(\"\\nEstoque total por tipo de produto:\")\n",
    "print(estoque_por_tipo_produto)\n",
    "\n",
    "#Estoque total por distrito\n",
    "estoque_por_distrito = df.groupby('distrito')['quantidade'].sum().reset_index()\n",
    "print(\"\\nEstoque total por distrito:\")\n",
    "print(estoque_por_distrito)\n",
    "\n",
    "#------------------INSIGHT OPCIONAIS-------------------\n",
    "# Insight 1: Unidades com Baixo Estoque\n",
    "# Definir um limite para estoque baixo, aqui definimos como 1000\n",
    "limite_estoque_baixo = 1000\n",
    "estoque_por_unidade = df.groupby('unidade')['quantidade'].sum().reset_index()\n",
    "\n",
    "# Insight 1.1: Identificar unidades com estoque abaixo do limite\n",
    "unidades_baixo_estoque = estoque_por_unidade[estoque_por_unidade['quantidade'] < limite_estoque_baixo]\n",
    "print(f\"\\nUnidades com estoque abaixo de {limite_estoque_baixo}:\")\n",
    "print(unidades_baixo_estoque)\n",
    "\n",
    "# Insight 2: Produtos com Estoque Crítico (Baixa Quantidade)\n",
    "# Definir um limite de 10 unidades para identificar produtos com estoque crítico\n",
    "limite_estoque_critico = 10\n",
    "produtos_criticos = df[df['quantidade'] < limite_estoque_critico]\n",
    "print(f\"\\nProdutos com estoque crítico (menos de {limite_estoque_critico} unidades):\")\n",
    "print(produtos_criticos[['unidade', 'produto', 'quantidade']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#(Não precisa fazer caso faça no BI) **#Salva o arquivo Excel contendo os dados tratados e os insights derivados em único arquivo em abas separadas.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalar o pacote necessário\n",
    "!pip install xlsxwriter\n",
    "# Etapa: Gerar colunas derivadas e salvar no arquivo principal\n",
    "\n",
    "# Estoque total por unidade\n",
    "estoque_por_unidade = df.groupby('unidade')['quantidade'].sum().reset_index()\n",
    "\n",
    "# Quantidade total de produtos por classe\n",
    "qtd_total_produtosPorclasse = df.groupby('classe')['quantidade'].sum().reset_index()\n",
    "\n",
    "# Quantidade total por tipo de produto\n",
    "estoque_por_tipo_produto = df.groupby('tipo_produto')['quantidade'].sum().reset_index()\n",
    "\n",
    "# Estoque total por distrito\n",
    "estoque_por_distrito = df.groupby('distrito')['quantidade'].sum().reset_index()\n",
    "\n",
    "# Insight: Unidades com estoque abaixo de 1000\n",
    "limite_estoque_baixo = 1000\n",
    "unidades_baixo_estoque = estoque_por_unidade[estoque_por_unidade['quantidade'] < limite_estoque_baixo]\n",
    "\n",
    "# Insight: Produtos com estoque crítico (menos de 10 unidades)\n",
    "limite_estoque_critico = 10\n",
    "produtos_criticos = df[df['quantidade'] < limite_estoque_critico][['unidade', 'produto', 'quantidade']]\n",
    "\n",
    "# Salvar o arquivo com as colunas derivadas\n",
    "\n",
    "# Importante: Se estiver utilizando no Google Colab, é necessário ter o Google Drive montado.\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Salvar os dados tratados em um arquivo Excel\n",
    "with pd.ExcelWriter('/content/drive/MyDrive/Colab Notebooks/Medicamentos/dados_tratados_com_insight.xlsx', engine='xlsxwriter') as writer:\n",
    "    df.to_excel(writer, sheet_name='Dados Originais', index=False)\n",
    "    estoque_por_unidade.to_excel(writer, sheet_name='Estoque por Unidade', index=False)\n",
    "    qtd_total_produtosPorclasse.to_excel(writer, sheet_name='Estoque por Classe', index=False)\n",
    "    estoque_por_tipo_produto.to_excel(writer, sheet_name='Estoque por Tipo Produto', index=False)\n",
    "    estoque_por_distrito.to_excel(writer, sheet_name='Estoque por Distrito', index=False)\n",
    "    unidades_baixo_estoque.to_excel(writer, sheet_name='Unidades Baixo Estoque', index=False)\n",
    "    produtos_criticos.to_excel(writer, sheet_name='Produtos Críticos', index=False)\n",
    "\n",
    "print(\"Arquivo Excel com as colunas derivadas foi salvo com sucesso.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#(Não precisa executar caso faça no BI) 8 Etapa de Carregamento  e Construção de Colunas derivadas - salva todos os insights em um único arquivo Excel sem o dataframe original\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalar o pacote necessário\n",
    "!pip install xlsxwriter\n",
    "\n",
    "# Etapa 9: Análise de Estoque por Unidade, Classe e Tipo de Produto\n",
    "\n",
    "# Estoque total por unidade\n",
    "estoque_por_unidade = df.groupby('unidade')['quantidade'].sum().reset_index()\n",
    "print(\"\\nEstoque total por unidade:\")\n",
    "print(estoque_por_unidade)\n",
    "\n",
    "# Quantidade total de produtos por classe\n",
    "qtd_total_produtosPorclasse = df.groupby('classe')['quantidade'].sum().reset_index()\n",
    "print(\"\\nEstoque total por classe:\")\n",
    "print(qtd_total_produtosPorclasse)\n",
    "\n",
    "# Quantidade total por tipo de produto\n",
    "estoque_por_tipo_produto = df.groupby('tipo_produto')['quantidade'].sum().reset_index()\n",
    "print(\"\\nEstoque total por tipo de produto:\")\n",
    "print(estoque_por_tipo_produto)\n",
    "\n",
    "# Estoque total por distrito\n",
    "estoque_por_distrito = df.groupby('distrito')['quantidade'].sum().reset_index()\n",
    "print(\"\\nEstoque total por distrito:\")\n",
    "print(estoque_por_distrito)\n",
    "\n",
    "# ------------------INSIGHT OPCIONAIS-------------------\n",
    "# Insight 1: Unidades com Baixo Estoque\n",
    "# Definir um limite para estoque baixo, aqui definimos como 1000\n",
    "limite_estoque_baixo = 1000\n",
    "estoque_por_unidade = df.groupby('unidade')['quantidade'].sum().reset_index()\n",
    "\n",
    "# Insight 1.1: Identificar unidades com estoque abaixo do limite\n",
    "unidades_baixo_estoque = estoque_por_unidade[estoque_por_unidade['quantidade'] < limite_estoque_baixo]\n",
    "print(f\"\\nUnidades com estoque abaixo de {limite_estoque_baixo}:\")\n",
    "print(unidades_baixo_estoque)\n",
    "\n",
    "# Insight 2: Produtos com Estoque Crítico (Baixa Quantidade)\n",
    "# Definir um limite de 10 unidades para identificar produtos com estoque crítico\n",
    "limite_estoque_critico = 10\n",
    "produtos_criticos = df[df['quantidade'] < limite_estoque_critico]\n",
    "print(f\"\\nProdutos com estoque crítico (menos de {limite_estoque_critico} unidades):\")\n",
    "print(produtos_criticos[['unidade', 'produto', 'quantidade']])\n",
    "\n",
    "# ------------------Salvando os Insights no Excel-------------------\n",
    "import pandas as pd\n",
    "\n",
    "# Caminho do arquivo Excel no Google Drive\n",
    "output_path = '/content/drive/MyDrive/Colab Notebooks/Medicamentos/insights_estoque_medicamentos.xlsx'\n",
    "\n",
    "# Criar um arquivo Excel com várias abas\n",
    "with pd.ExcelWriter(output_path, engine='xlsxwriter') as writer:\n",
    "    # Aba 1: Estoque total por unidade\n",
    "    estoque_por_unidade.to_excel(writer, sheet_name='Estoque por Unidade', index=False)\n",
    "\n",
    "    # Aba 2: Quantidade total por classe\n",
    "    qtd_total_produtosPorclasse.to_excel(writer, sheet_name='Estoque por Classe', index=False)\n",
    "\n",
    "    # Aba 3: Quantidade total por tipo de produto\n",
    "    estoque_por_tipo_produto.to_excel(writer, sheet_name='Estoque por Tipo Produto', index=False)\n",
    "\n",
    "    # Aba 4: Estoque total por distrito\n",
    "    estoque_por_distrito.to_excel(writer, sheet_name='Estoque por Distrito', index=False)\n",
    "\n",
    "    # Aba 5: Unidades com estoque baixo\n",
    "    unidades_baixo_estoque.to_excel(writer, sheet_name='Unidades Baixo Estoque', index=False)\n",
    "\n",
    "    # Aba 6: Produtos com estoque crítico\n",
    "    produtos_criticos[['unidade', 'produto', 'quantidade']].to_excel(writer, sheet_name='Produtos Estoque Crítico', index=False)\n",
    "\n",
    "print(f\"Arquivo Excel com os insights foi salvo em: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#9-Etapa de Carregamento - Salvamento do dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Salvar o arquivo CSV no Google Drive, usando o DataFrame 'df'\n",
    "df.to_excel('/content/drive/MyDrive/Colab Notebooks/Medicamentos/dados_tratados_sem_insight.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
